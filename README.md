# Bayesian Machine Learning Project - MVA Master - Eug√®ne Berta

This repository contains my project on the paper: Variational Refinement for Importance Sampling Using the Forward Kullback-Leibler Divergence

By: Ghassen Jerfel, Serena Wang, Clara Fannjiang, Katherine A. Heller, Yian Ma, Michael I. Jordan

Available at: https://arxiv.org/abs/2106.15980v1

- Report.pdf is the project report associated with this repository.
- requirements.txt contains the list of dependencies to run the code.
- utils.py contains the FKL and RKL losses described in the paper and classes to handle multivariate normal distributions and Gaussian Mixture Models in our experiments. Each class/function has detailed docstrings and comments to help understand our implementation.
- ImportanceSamplingFKL.ipynb contains all our experiments as described in the report. Notice that the results of our experiments can vary from one run to the next as the losses we use are stochastic estimators with high variance. We tried to take this into account in our interpretations. In any case, you should be able to obtain similar results to ours with at most a few runs.

Don't hesitate to contact me at eugene[dot]berta[at]gmail.com, I would be happy to answer any question.

## Running the code

You can install the dependencies using:

```
conda create -n <environment-name>
conda activate <environment-name>
pip3 install -r requirements.txt
```
